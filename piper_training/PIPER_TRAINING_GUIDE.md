# ğŸ“š Piper TTS Training - Guida Completa

## ğŸ“‹ Indice
- [Introduzione](#introduzione)
- [Requisiti](#requisiti)
- [Preparazione Dataset](#preparazione-dataset)
- [Training vs Fine-Tuning](#training-vs-fine-tuning)
- [Configurazione Training](#configurazione-training)
- [Processo di Training](#processo-di-training)
- [Troubleshooting](#troubleshooting)
- [Best Practices](#best-practices)
- [FAQ](#faq)

---

## ğŸ¯ Introduzione

Piper Ã¨ un TTS (Text-to-Speech) veloce e locale basato su VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech).

**Vantaggi:**
- âœ… Completamente offline
- âœ… Veloce (real-time su CPU)
- âœ… QualitÃ  eccellente
- âœ… Multi-lingua
- âœ… Open source

**Quando usare Training vs Fine-Tuning:**

| Caratteristica | Training da Zero | Fine-Tuning |
|---------------|------------------|-------------|
| Dataset minimo | 2+ ore | 30+ minuti |
| Tempo training | 12-16 ore | 8-12 ore |
| Controllo | Massimo | Medio |
| DifficoltÃ  | Alta | Media |
| Caso d'uso | Nuove lingue/accenti | Adattare voce esistente |

---

## ğŸ’» Requisiti

### Hardware
- **GPU:** NVIDIA con almeno 8GB VRAM (consigliato: T4, V100, A100)
- **RAM:** 16GB+ consigliati
- **Storage:** 10GB+ liberi

### Software
- Python 3.8+
- CUDA 11.x o superiore
- Git

### Google Colab
- Account Google
- Colab Pro consigliato per training lunghi (evita disconnessioni)

---

## ğŸ“ Preparazione Dataset

### Struttura Richiesta

```
my_dataset/
â”œâ”€â”€ wavs/              # File audio WAV
â”‚   â”œâ”€â”€ audio_001.wav
â”‚   â”œâ”€â”€ audio_002.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.csv       # Trascrizioni
```

### Requisiti Audio

**Formato:**
- **Sample rate:** 22050 Hz (consigliato) o 16000 Hz
- **Bit depth:** 16-bit
- **Canali:** Mono
- **Formato:** WAV non compresso

**Conversione con ffmpeg:**
```bash
# Converti singolo file
ffmpeg -i input.mp3 -ar 22050 -ac 1 -c:a pcm_s16le output.wav

# Batch conversion (tutti i file in una cartella)
for file in *.mp3; do
    ffmpeg -i "$file" -ar 22050 -ac 1 -c:a pcm_s16le "wavs/${file%.mp3}.wav"
done
```

### Formato metadata.csv

**Struttura:** `filename|trascrizione`

**Esempio:**
```
audio_001|Questa Ã¨ la prima frase di esempio.
audio_002|La qualitÃ  del dataset determina la qualitÃ  del modello.
audio_003|Ogni frase deve essere trascritta accuratamente.
```

**IMPORTANTE:**
- âŒ NO header nel file
- âŒ NO estensione .wav nel filename
- âœ… Separatore: pipe `|`
- âœ… Trascrizioni accurate (no errori di battitura)
- âœ… Punteggiatura corretta

### QualitÃ  Dataset

**Requisiti essenziali:**
- âœ… Audio pulito (no rumore di fondo)
- âœ… Volume consistente tra i file
- âœ… Singolo speaker per modello
- âœ… Trascrizioni precise (100% match con audio)

**Dimensione dataset:**

| Durata | QualitÃ  Risultato | Note |
|--------|------------------|------|
| < 30 min | Scarsa | Training possibile ma risultati limitati |
| 30 min - 1 ora | Accettabile | Fine-tuning OK |
| 1-2 ore | Buona | Training da zero OK |
| 2-5 ore | Ottima | QualitÃ  professionale |
| 5+ ore | Eccellente | Massima qualitÃ  |

### Tools per Preparazione Dataset

**LJSpeech-style dataset:**
```python
# Script per creare metadata.csv da cartella audio
import os
import glob

audio_files = sorted(glob.glob("wavs/*.wav"))
with open("metadata.csv", "w") as f:
    for audio_path in audio_files:
        filename = os.path.basename(audio_path).replace(".wav", "")
        # âš ï¸ Inserisci trascrizione manuale o da ASR
        transcription = "TRASCRIZIONE QUI"
        f.write(f"{filename}|{transcription}\n")
```

**Trascrizione automatica (ASR):**
```python
# Usa Whisper per trascrizioni automatiche
import whisper

model = whisper.load_model("base")

for audio_file in glob.glob("wavs/*.wav"):
    result = model.transcribe(audio_file)
    print(f"{audio_file}: {result['text']}")
```

---

## ğŸ›ï¸ Training vs Fine-Tuning

### Training da Zero

**Quando usarlo:**
- Lingua/accento non supportato
- Vuoi massimo controllo
- Hai 2+ ore di audio

**Pro:**
- Controllo totale su architettura
- Ottimizzato per il tuo dataset
- Nessuna dipendenza da modelli pre-esistenti

**Contro:**
- Richiede piÃ¹ dati
- Training piÃ¹ lungo
- PiÃ¹ complesso da configurare

**Notebook:** `Piper_Training_Complete.ipynb`

---

### Fine-Tuning

**Quando usarlo:**
- Vuoi adattare voce esistente
- Hai 30-60 minuti di audio
- Vuoi risultati piÃ¹ rapidi

**Pro:**
- Richiede meno dati
- Training piÃ¹ veloce
- Setup piÃ¹ semplice

**Contro:**
- Limitato dalla base del modello pre-addestrato
- Meno flessibilitÃ 

**Notebook:** `Piper_FineTuning_Colab.ipynb`

---

## âš™ï¸ Configurazione Training

### File config.json

```json
{
  "audio": {
    "sample_rate": 22050,
    "max_wav_value": 32767.0,
    "filter_length": 1024,
    "hop_length": 256,
    "win_length": 1024
  },
  "model": {
    "name": "vits",
    "hidden_channels": 192,
    "inter_channels": 192,
    "filter_channels": 768,
    "n_heads": 2,
    "n_layers": 6,
    "kernel_size": 3,
    "p_dropout": 0.1
  },
  "training": {
    "epochs": 10000,
    "learning_rate": 0.0002,
    "batch_size": 16,
    "log_interval": 100,
    "save_interval": 1000,
    "num_workers": 4
  }
}
```

### Hyperparameters Spiegati

**Audio:**
- `sample_rate`: 22050 Hz (standard) o 16000 Hz (piÃ¹ veloce)
- `hop_length`: 256 (default, non modificare)

**Model:**
- `hidden_channels`: 192 (piÃ¹ alto = piÃ¹ qualitÃ  ma piÃ¹ lento)
- `n_layers`: 6 (piÃ¹ layer = modello piÃ¹ espressivo)
- `p_dropout`: 0.1 (previene overfitting)

**Training:**
- `epochs`: 10000 (training si ferma automaticamente quando converge)
- `learning_rate`: 0.0002 (0.0001-0.0003 Ã¨ range sicuro)
- `batch_size`: 16 (riduci se OOM, aumenta se GPU potente)

### Tuning per GPU

| GPU | Batch Size | Note |
|-----|-----------|------|
| T4 (16GB) | 16 | Default Colab |
| V100 (32GB) | 32 | Raddoppia velocitÃ  |
| A100 (40GB) | 64 | Massima velocitÃ  |
| < 12GB | 8 | Riduci se OOM |

---

## ğŸš€ Processo di Training

### 1. Preprocessing

```bash
python piper_train/preprocess.py \
    --input-dir /path/to/dataset \
    --output-dir /path/to/output \
    --language it-it \
    --sample-rate 22050
```

**Output:**
- Genera file `.npy` per ogni audio (mel-spectrograms)
- Crea phoneme vocabulary
- Valida il dataset

**Tempo:** 5-10 minuti per 1h di audio

---

### 2. Training

```bash
python -m piper_train \
    --dataset-dir /path/to/preprocessed \
    --output-dir /path/to/checkpoints \
    --config config.json \
    --restore-checkpoint  # Riprende da ultimo checkpoint
```

**Cosa succede:**
1. Carica dataset preprocessato
2. Inizializza modello VITS
3. Training loop:
   - Forward pass
   - Calcola loss (reconstruction + adversarial)
   - Backward pass
   - Update weights
4. Salva checkpoint ogni N steps

**Monitoraggio:**
```
[Epoch 1000/10000] Loss: 2.345 | Duration: 1.23s/batch
[Epoch 2000/10000] Loss: 1.876 | Duration: 1.21s/batch
[Epoch 3000/10000] Loss: 1.542 | Duration: 1.19s/batch
```

**Loss atteso:**
- Inizio: ~3.0-5.0
- Dopo 2000 epochs: ~1.5-2.0
- Fine training: ~0.8-1.2

---

### 3. Export ONNX

```bash
python -m piper_train.export_onnx \
    checkpoints/checkpoint_10000.pt \
    model.onnx
```

**Output:**
- `model.onnx`: Modello pronto per inferenza
- `model.onnx.json`: Configurazione

**Dimensione file:** ~10-50MB a seconda della configurazione

---

## ğŸ› Troubleshooting

### OOM (Out of Memory)

**Sintomo:**
```
RuntimeError: CUDA out of memory
```

**Soluzioni:**
1. **Riduci batch_size:**
   ```json
   "batch_size": 8  // invece di 16
   ```

2. **Riduci model complexity:**
   ```json
   "hidden_channels": 128,  // invece di 192
   "n_layers": 4           // invece di 6
   ```

3. **Usa GPU piÃ¹ potente:** Colab Pro con A100

---

### Audio Distorto

**Sintomo:** Output audio Ã¨ metallico, distorto, o incomprensibile

**Cause comuni:**
1. **Sample rate errato:**
   - Verifica tutti WAV siano 22050Hz
   ```bash
   soxi wavs/*.wav | grep "Sample Rate"
   ```

2. **Dataset rumoroso:**
   - Pulisci audio con noise reduction
   - Rimuovi file con troppo rumore di fondo

3. **Training insufficiente:**
   - Continua training per piÃ¹ epochs
   - Verifica loss sia < 1.5

---

### Training Troppo Lento

**Sintomo:** < 0.5 batch/sec

**Soluzioni:**
1. **Verifica GPU:**
   ```python
   !nvidia-smi
   ```
   Deve mostrare utilizzo GPU ~90-100%

2. **Aumenta num_workers:**
   ```json
   "num_workers": 8  // invece di 4
   ```

3. **Usa sample_rate piÃ¹ basso:**
   ```json
   "sample_rate": 16000  // invece di 22050
   ```

---

### Loss Non Converge

**Sintomo:** Loss rimane > 2.0 dopo 5000 epochs

**Cause:**
1. **Learning rate troppo alto/basso:**
   - Prova: 0.0001, 0.0002, 0.0003

2. **Dataset problematico:**
   - Verifica metadata.csv (no errori)
   - Controlla audio (no silenzi lunghi)

3. **Config errata:**
   - Usa config di default
   - Non modificare parametri audio

---

### Checkpoint Corrotti

**Sintomo:**
```
Error loading checkpoint
```

**Soluzione:**
- Usa checkpoint precedente:
  ```bash
  python -m piper_train \
      --restore-checkpoint checkpoints/checkpoint_9000.pt
  ```

---

## âœ… Best Practices

### Dataset
- âœ… 1 speaker per modello
- âœ… Audio consistente (stesso microfono, ambiente)
- âœ… Trascrizioni 100% accurate
- âœ… Frasi varie (domande, esclamazioni, affermazioni)
- âŒ NO rumore di fondo
- âŒ NO silenzios lunghi (>2s)
- âŒ NO audio troppo corti (<1s)

### Training
- âœ… Monitora loss regolarmente
- âœ… Salva checkpoint ogni 1000 epochs
- âœ… Testa qualitÃ  ogni 2000 epochs
- âœ… Usa Colab Pro per training lunghi
- âŒ NO interruzioni frequenti
- âŒ NO modifiche config durante training

### Testing
- âœ… Testa con frasi DIVERSE dal training
- âœ… Varia lunghezza frasi (corte, medie, lunghe)
- âœ… Testa punteggiatura (!, ?, .)
- âœ… Chiedi feedback a terzi

---

## â“ FAQ

### Q: Quanto dataset serve davvero?
**A:** Dipende:
- Fine-tuning: 30-60 min OK
- Training da zero: 2+ ore consigliato
- QualitÃ  professionale: 5+ ore

---

### Q: Posso usare dataset multi-speaker?
**A:** No, 1 speaker per modello. Se hai multi-speaker:
1. Separa audio per speaker
2. Addestra 1 modello per speaker

---

### Q: Come miglioro qualitÃ  audio?
**A:**
1. Usa microfono decente (no laptop mic)
2. Registra in ambiente silenzioso
3. Normalizza volume
4. Rimuovi rumore con Audacity/RX

---

### Q: Training si Ã¨ interrotto, cosa faccio?
**A:** Riprendi da ultimo checkpoint:
```bash
python -m piper_train \
    --restore-checkpoint  # Auto-trova ultimo checkpoint
```

---

### Q: Posso usare GPU locale invece di Colab?
**A:** SÃ¬! Setup:
```bash
git clone https://github.com/rhasspy/piper
cd piper/src/python
pip install -e .
# Poi segui stessi passi del notebook
```

---

### Q: Come confronto qualitÃ  tra checkpoint?
**A:**
```python
# Test ogni checkpoint
for ckpt in checkpoints/*.pt:
    export_onnx(ckpt, f"models/test_{ckpt}.onnx")
    generate_sample(f"models/test_{ckpt}.onnx", "Frase di test")
    # Ascolta e confronta
```

---

### Q: Loss Ã¨ basso ma audio Ã¨ brutto, perchÃ©?
**A:** Loss basso â‰  qualitÃ  alta. Cause:
1. Overfitting: modello "memorizza" invece di "imparare"
   - Soluzione: Aumenta dataset
2. Dataset problematico (es. rumore)
   - Soluzione: Pulisci dataset
3. Config errata
   - Soluzione: Usa config di default

---

## ğŸ“š Risorse

### Documentazione
- [Piper GitHub](https://github.com/rhasspy/piper)
- [VITS Paper](https://arxiv.org/abs/2106.06103)
- [Piper Training Guide (EN)](https://github.com/rhasspy/piper/blob/master/TRAINING.md)

### Community
- [Piper Discussions](https://github.com/rhasspy/piper/discussions)
- [Home Assistant Forum](https://community.home-assistant.io/)

### Tools
- [Audacity](https://www.audacityteam.org/) - Audio editing
- [Whisper](https://github.com/openai/whisper) - Trascrizione automatica
- [FFmpeg](https://ffmpeg.org/) - Conversione audio

---

## ğŸ¯ Prossimi Passi

Dopo aver completato il training:

1. **Test approfondito:**
   - Prova 20+ frasi diverse
   - Varia punteggiatura e intonazione
   - Chiedi feedback

2. **Ottimizzazione:**
   - Se non sei soddisfatto, aumenta dataset
   - Prova diversi learning rates
   - Continua training per piÃ¹ epochs

3. **Deploy:**
   - Integra in applicazione
   - Usa `piper` CLI per inferenza
   - Considera export per mobile (TFLite)

4. **Condivisione:**
   - Pubblica modello (se open dataset)
   - Contribuisci a Piper community
   - Documenta processo

---

**Buon training! ğŸš€**

Per domande: apri issue su [Piper GitHub](https://github.com/rhasspy/piper/issues)
